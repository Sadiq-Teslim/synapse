# Synapse Chronos - 3D World Integration Plan

## The Vision ðŸŽ®

**"I just want to walk in my garden one last time."**

Transform stroke rehabilitation from boring repetitive exercises into an emotional journey through the patient's own memories, reconstructed as immersive 3D worlds.

## Core Concept

1. **Patient uploads 5-10 photos** of meaningful places (childhood village, wedding venue, favorite park)
2. **AI reconstructs 2D photos into 3D immersive worlds** using 3D Gaussian Splatting or NeRFs
3. **Patient controls movement through 3D world** by performing rehab exercises
   - High-knee step â†’ Move forward in memory
   - Torso rotation â†’ Look around
   - Arm raises â†’ Unlock audio memories

## Tech Stack

### Option A: 3D Gaussian Splatting (Advanced)
- **Backend**: Python + Azure ML
- **Process**: 
  1. User uploads photos/video
  2. Send to Azure ML endpoint
  3. Run 3DGS training (open source, fast)
  4. Stream splat file back to browser
- **Rendering**: Three.js + gsplat.js (WebGL)

### Option B: API-Based (Easier - Recommended for MVP)
- **Blockade Labs Skybox AI API**: Generate 360Â° worlds from text prompts
- **Luma AI API**: Generate 3D scenes from images
- **Process**:
  1. User uploads photo
  2. Send to API with prompt ("A peaceful village in Lagos, 1990")
  3. Receive 360Â° panorama or 3D scene
  4. Render in Three.js

### Option C: Unity WebGL (Most Polished)
- **Unity with Gaussian Splatting plugin**
- Export as WebGL build
- Embed in Next.js app
- Communicate via WebSocket/PostMessage

## Implementation Phases

### Phase 1: Photo Upload & Processing
- [ ] Add photo upload component to onboarding
- [ ] Store photos in localStorage (or Azure Blob Storage)
- [ ] Create "Memory Gallery" screen
- [ ] Integrate Blockade Labs API for 360Â° generation

### Phase 2: 3D World Renderer
- [ ] Set up Three.js in React
- [ ] Create 360Â° panorama viewer
- [ ] Add camera controls (initially mouse/touch)
- [ ] Add basic lighting and atmosphere

### Phase 3: Bio-Controller Integration
- [ ] Map pose detection to camera movement
  - Right knee Y position â†’ Forward movement
  - Left arm angle â†’ Look left/right
  - Torso rotation â†’ Camera rotation
- [ ] Add movement thresholds
- [ ] Smooth camera interpolation

### Phase 4: Exercise-to-Movement Mapping
- [ ] High-knee step detection â†’ Forward movement
- [ ] Torso rotation â†’ Camera pan
- [ ] Arm raises â†’ Unlock points of interest
- [ ] Add visual feedback (progress bar, distance traveled)

### Phase 5: Audio Memories (Optional)
- [ ] Add audio recording/upload
- [ ] Trigger audio at specific locations
- [ ] "This is where we got married..."

### Phase 6: Vitals Monitoring (Future)
- [ ] Integrate rPPG (Remote Photoplethysmography)
- [ ] Monitor heart rate via camera
- [ ] Alert if over-exertion detected

## File Structure

```
synapse-ai-web/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ memories/
â”‚   â”‚   â”œâ”€â”€ page.tsx          # Memory gallery
â”‚   â”‚   â””â”€â”€ [id]/
â”‚   â”‚       â””â”€â”€ page.tsx      # 3D world view
â”‚   â””â”€â”€ exercise/
â”‚       â””â”€â”€ page.tsx          # Updated with 3D world option
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ MemoryUpload.tsx      # Photo upload component
â”‚   â”œâ”€â”€ MemoryGallery.tsx     # List of memories
â”‚   â”œâ”€â”€ World3D.tsx           # Three.js 3D world renderer
â”‚   â”œâ”€â”€ BioController.tsx      # Pose-to-movement mapper
â”‚   â””â”€â”€ VitalsMonitor.tsx     # rPPG heart rate (future)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ worldGenerator.ts     # API calls to Blockade/Luma
â”‚   â”œâ”€â”€ bioController.ts      # Pose-to-movement logic
â”‚   â””â”€â”€ threejsSetup.ts       # Three.js initialization
â””â”€â”€ types/
    â””â”€â”€ memory.ts             # Memory/World types
```

## API Integration

### Blockade Labs Skybox AI
```typescript
const generateWorld = async (prompt: string) => {
  const response = await fetch('https://api.blockadelabs.com/api/v1/skybox', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ prompt })
  });
  return response.json();
};
```

### Luma AI (Alternative)
```typescript
const generateScene = async (image: File) => {
  const formData = new FormData();
  formData.append('image', image);
  
  const response = await fetch('https://api.lumalabs.ai/v1/scenes', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${API_KEY}`
    },
    body: formData
  });
  return response.json();
};
```

## Pose-to-Movement Mapping

```typescript
// Example: High-knee step â†’ Forward movement
if (rightKneeY > threshold && !isMoving) {
  moveCameraForward(1.0); // Move 1 meter forward
  isMoving = true;
}

// Torso rotation â†’ Camera pan
const torsoAngle = calculateTorsoRotation(pose);
camera.rotateY(torsoAngle * 0.01);

// Arm raise â†’ Unlock POI
if (leftArmAngle < 90 && !poiUnlocked) {
  unlockPointOfInterest();
  playAudioMemory();
}
```

## Next Steps

1. **Start with Blockade Labs API** (easiest)
2. **Create photo upload flow** in onboarding
3. **Build basic 360Â° viewer** with Three.js
4. **Connect pose detection** to camera movement
5. **Test with real exercises**

## Resources

- [Blockade Labs API Docs](https://docs.blockadelabs.com/)
- [Three.js Documentation](https://threejs.org/docs/)
- [3D Gaussian Splatting Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)
- [gsplat.js (WebGL renderer)](https://github.com/mkkellogg/GaussianSplats3D)

